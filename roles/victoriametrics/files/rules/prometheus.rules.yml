# Warning: File is managed by Ansible
groups:
  - name: prometheus self monitoring rules
    rules:
      # - alert: missing team_label
      #   expr: "ALERTS{team=''}"
      #   for: 5m
      #   labels:
      #     team: operations
      #     slack_channel: prometheus_operations
      #     severity: warning
      #   annotations:
      #     description: "team label is missing on alert {{$labels.alertname}} (instance {{$labels.instance}})"
      #     summary: "missing team label"

      # - alert: missing alert_label
      #   expr: "ALERTS{email_to=''} AND ALERTS{slack_channel=''} AND ALERTS{disable_alerts!='True'}"
      #   for: 5m
      #   labels:
      #     team: operations
      #     slack_channel: prometheus_operations
      #     severity: warning
      #   annotations:
      #     description: "email_to or slack_channel label is missing on {{$labels.alertname}} (instance {{$labels.instance}})"
      #     summary: "missing label email_to or slack_channel"

      - alert: PrometheusConfigReloadFailed
        annotations:
          summary: Reloading Prometheus' configuration failed
          description: >-
            Reloading Prometheus' configuration has failed for
            {{ $labels.namespace }}/{{ $labels.pod }}
        expr: |
          prometheus_config_last_reload_successful{job=~"prometheus(-k8s)?"} == 0
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          summary: Prometheus' alert notification queue is running full
          description: >-
            Prometheus' alert notification queue is running full for
            {{ $labels.namespace }}/{{ $labels.pod }}
        expr: >
          predict_linear(prometheus_notifications_queue_length{job=~"prometheus(-k8s)?"}[5m],
          60 * 30) >
          prometheus_notifications_queue_capacity{job=~"prometheus(-k8s)?"}
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusErrorSendingAlerts
        annotations:
          summary: Errors while sending alert from Prometheus
          description: >-
            Errors while sending alerts from Prometheus
            {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager
            {{ $labels.Alertmanager }}
        expr: >
          rate(prometheus_notifications_errors_total{job=~"prometheus(-k8s)?"}[5m])
          /
          rate(prometheus_notifications_sent_total{job=~"prometheus(-k8s)?"}[5m])
          > 0.01
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusErrorSendingAlerts
        annotations:
          summary: Errors while sending alerts from Prometheus
          description: >-
            Errors while sending alerts from Prometheus
            {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager
            {{ $labels.Alertmanager }}
        expr: >
          rate(prometheus_notifications_errors_total{job=~"prometheus(-k8s)?"}[5m])
          /
          rate(prometheus_notifications_sent_total{job=~"prometheus(-k8s)?"}[5m])
          > 0.03
        for: 10m
        labels:
          severity: critical

      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          summary: Prometheus is not connected to any Alertmanagers
          description: >-
            Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not
            connected to any Alertmanagers
        expr: >
          prometheus_notifications_alertmanagers_discovered{job=~"prometheus(-k8s)?"}
          < 1
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusTSDBReloadsFailing
        annotations:
          summary: Prometheus has issues reloading data blocks from disk
          description: >-
            {{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }}
            reload failures over the last four hours.
        expr: >
          increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus(-k8s)?"}[2h])
          > 0
        for: 12h
        labels:
          severity: warning

      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          summary: Prometheus has issues compacting sample blocks
          description: >-
            {{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }}
            compaction failures over the last four hours.
        expr: >
          increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus(-k8s)?"}[2h])
          > 0
        for: 12h
        labels:
          severity: warning

      - alert: PrometheusTSDBWALCorruptions
        annotations:
          summary: Prometheus write-ahead log is corrupted
          description: >-
            {{ $labels.job }} at {{ $labels.instance }} has a corrupted
            write-ahead log (WAL).
        expr: |
          tsdb_wal_corruptions_total{job=~"prometheus(-k8s)?"} > 0
        for: 4h
        labels:
          severity: warning

      - alert: PrometheusNotIngestingSamples
        annotations:
          summary: Prometheus isn't ingesting samples
          description: >-
            Prometheus {{ $labels.namespace }}/{{ $labels.pod }} isn't
            ingesting samples.
        expr: >
          rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus(-k8s)?"}[5m])
          <= 0
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusTargetScrapesDuplicate
        annotations:
          summary: Prometheus has many samples rejected
          description: >-
            {{ $labels.namespace }}/{{ $labels.pod }} has many samples rejected
            due to duplicate timestamps but different values
        expr: >
          increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus(-k8s)?"}[5m])
          > 0
        for: 10m
        labels:
          severity: warning

  - name: alertmanager self monitoring rules
    rules:
      - alert: NotificationFailure
        expr: "increase(alertmanager_notifications_failed_total[10m]) > 0"
        # for: 1m
        labels:
          team: operations
          severity: critical
          alert_channel: fallback
        annotations:
          summary: "Alertmanager Notification Failure"
          description: "Alertmanager could not send {{ $value }} alerts to '{{ $labels.integration }}' (instance {{ $labels.instance }})"
