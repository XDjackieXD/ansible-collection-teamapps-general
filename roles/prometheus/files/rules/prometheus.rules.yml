# Warning: File is managed by Ansible
groups:
  - name: prometheus self monitoring rules
    rules:
      # - alert: missing team_label
      #   expr: "ALERTS{team=''}"
      #   for: 5m
      #   labels:
      #     team: operations
      #     slack_channel: prometheus_operations
      #     severity: warning
      #   annotations:
      #     description: "team label is missing on alert {{$labels.alertname}} (instance {{$labels.instance}})"
      #     summary: "missing team label"

      # - alert: missing alert_label
      #   expr: "ALERTS{email_to=''} AND ALERTS{slack_channel=''} AND ALERTS{disable_alerts!='True'}"
      #   for: 5m
      #   labels:
      #     team: operations
      #     slack_channel: prometheus_operations
      #     severity: warning
      #   annotations:
      #     description: "email_to or slack_channel label is missing on {{$labels.alertname}} (instance {{$labels.instance}})"
      #     summary: "missing label email_to or slack_channel"

      - alert: PrometheusConfigReloadFailed
        annotations:
          description: >-
            Reloading Prometheus' configuration has failed for
            {{$labels.namespace}}/{{$labels.pod}}
          summary: Reloading Promehteus' configuration failed
        expr: |
          prometheus_config_last_reload_successful{job=~"prometheus(-k8s)?"} == 0
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: >-
            Prometheus' alert notification queue is running full for
            {{$labels.namespace}}/{{ $labels.pod}}
          summary: Prometheus' alert notification queue is running full
        expr: >
          predict_linear(prometheus_notifications_queue_length{job=~"prometheus(-k8s)?"}[5m],
          60 * 30) >
          prometheus_notifications_queue_capacity{job=~"prometheus(-k8s)?"}
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: >-
            Errors while sending alerts from Prometheus
            {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager
            {{$labels.Alertmanager}}
          summary: Errors while sending alert from Prometheus
        expr: >
          rate(prometheus_notifications_errors_total{job=~"prometheus(-k8s)?"}[5m])
          /
          rate(prometheus_notifications_sent_total{job=~"prometheus(-k8s)?"}[5m])
          > 0.01
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusErrorSendingAlerts
        annotations:
          description: >-
            Errors while sending alerts from Prometheus
            {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager
            {{$labels.Alertmanager}}
          summary: Errors while sending alerts from Prometheus
        expr: >
          rate(prometheus_notifications_errors_total{job=~"prometheus(-k8s)?"}[5m])
          /
          rate(prometheus_notifications_sent_total{job=~"prometheus(-k8s)?"}[5m])
          > 0.03
        for: 10m
        labels:
          severity: critical

      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: >-
            Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not
            connected to any Alertmanagers
          summary: Prometheus is not connected to any Alertmanagers
        expr: >
          prometheus_notifications_alertmanagers_discovered{job=~"prometheus(-k8s)?"}
          < 1
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: >-
            {{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            reload failures over the last four hours.
          summary: Prometheus has issues reloading data blocks from disk
        expr: >
          increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus(-k8s)?"}[2h])
          > 0
        for: 12h
        labels:
          severity: warning

      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: >-
            {{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            compaction failures over the last four hours.
          summary: Prometheus has issues compacting sample blocks
        expr: >
          increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus(-k8s)?"}[2h])
          > 0
        for: 12h
        labels:
          severity: warning

      - alert: PrometheusTSDBWALCorruptions
        annotations:
          description: >-
            {{$labels.job}} at {{$labels.instance}} has a corrupted
            write-ahead log (WAL).
          summary: Prometheus write-ahead log is corrupted
        expr: |
          tsdb_wal_corruptions_total{job=~"prometheus(-k8s)?"} > 0
        for: 4h
        labels:
          severity: warning

      - alert: PrometheusNotIngestingSamples
        annotations:
          description: >-
            Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't
            ingesting samples.
          summary: Prometheus isn't ingesting samples
        expr: >
          rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus(-k8s)?"}[5m])
          <= 0
        for: 10m
        labels:
          severity: warning

      - alert: PrometheusTargetScrapesDuplicate
        annotations:
          description: >-
            {{$labels.namespace}}/{{$labels.pod}} has many samples rejected
            due to duplicate timestamps but different values
          summary: Prometheus has many samples rejected
        expr: >
          increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus(-k8s)?"}[5m])
          > 0
        for: 10m
        labels:
          severity: warning
